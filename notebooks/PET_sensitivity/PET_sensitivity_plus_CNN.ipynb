{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on demonstration of maximum-likelihood reconstruction with SIRF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook to get you used to some of the syntax of SIRF and PyTorch and introduce the ideas behind the sensitivity image and why we'd want to change it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% make sure figures appears inline and animations works\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup the working directory for the notebook\n",
    "import notebook_setup\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "cd_to_working_dir('PET', 'ML_reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Initial imports etc\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import affine_transform\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from numba import njit, prange\n",
    "#%% Read in images\n",
    "import brainweb\n",
    "from tqdm import tqdm\n",
    "#import scipy\n",
    "#from scipy import optimize\n",
    "import sirf.STIR as pet\n",
    "import sirf.Reg as reg\n",
    "from sirf.Utilities import examples_data_path\n",
    "from sirf_exercises import exercises_data_path\n",
    "pet.set_verbosity(0)\n",
    "\n",
    "# increase dpi of matplotlib figures\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "# define the directory with input files for this notebook\n",
    "data_path = os.path.join(examples_data_path('PET'), 'thorax_single_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up redirection of STIR messages to files\n",
    "msg_red = pet.MessageRedirector('info.txt', 'warnings.txt', 'errors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% some handy function definitions\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink=.4)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def make_positive(image_array):\n",
    "    \"\"\"truncate any negatives to zero\"\"\"\n",
    "    image_array[image_array<0] = 0\n",
    "    return image_array\n",
    "\n",
    "def make_cylindrical_FOV(image):\n",
    "    \"\"\"truncate to cylindrical FOV\"\"\"\n",
    "    filter = pet.TruncateToCylinderProcessor()\n",
    "    filter.apply(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some simulated data from ground-truth images\n",
    "\n",
    "We'll use brainweb's data base to simulate brain FDG-18 images here. Don't worry too much about this next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname, url= sorted(brainweb.utils.LINKS.items())[0]\n",
    "files = brainweb.get_file(fname, url, \".\")\n",
    "data = brainweb.load_file(fname)\n",
    "\n",
    "brainweb.seed(1337)\n",
    "\n",
    "for f in tqdm([fname], desc=\"mMR ground truths\", unit=\"subject\"):\n",
    "    vol = brainweb.get_mmr_fromfile(f, petNoise=1, t1Noise=0.75, t2Noise=0.75, petSigma=1, t1Sigma=1, t2Sigma=1)\n",
    "\n",
    "FDG_arr  = vol['PET']\n",
    "T1_arr   = vol['T1']\n",
    "uMap_arr = vol['uMap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img, cropx, cropy):\n",
    "    z,y,x = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[:, starty:starty+cropy,startx:startx+cropx]\n",
    "\n",
    "# function to zoom out and crop the brainweb images\n",
    "fdg_zoomed = crop(ndimage.zoom(FDG_arr, 1, order=1), 155, 155) # crop and zoom image\n",
    "fdg_zoomed = np.expand_dims(fdg_zoomed[50,:,:], axis = 0) # expand for pseudo 3D\n",
    "\n",
    "umap_zoomed = crop(ndimage.zoom(uMap_arr, 1, order=1), 155, 155) # crop and zoom image\n",
    "umap_zoomed = np.expand_dims(umap_zoomed[50,:,:], axis = 0) # expand for pseudo 3D\n",
    "\n",
    "image_template = pet.ImageData(os.path.join(data_path, 'emission.hv'))\n",
    "image = image_template.clone().fill(fdg_zoomed)\n",
    "attn_image = image_template.clone().fill(umap_zoomed)\n",
    "\n",
    "template = pet.AcquisitionData(os.path.join(data_path, 'template_sinogram.hs'))\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([1,2,1], image.as_array()[0,:], \"image\")\n",
    "plot_2d_image([1,2,2], attn_image.as_array()[0,:], \"attenuation image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save max for future displays\n",
    "cmax = image.max()*.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start creating our detector model. A PET (or SPECT or CT or...) acquisition process is characterised by a system matrix, $\\mathcal{A}$, as well as additive contributions consisting of scatter and random coincidences:\n",
    "$$f = \\mathcal{A}u + s + r $$\n",
    "Wwhere $f$ is our data, $u$ is our image and $s,r$ are our additive componatnts. We will only model the system matrix in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our system model is itself comprised of a number of different operations. We will concentrate on three of these: the radon transform, $\\mathcal{R}$, attenuation, $A$, and detector normalisation $\\mathcal{N}$, giving us:\n",
    "$$ \\mathcal{A} \\approx \\mathcal{N} A \\mathcal{R} $$\n",
    "where attenuation corrections and normalisation and multiplicative factors in the projection domain. \\\n",
    "And so our next job is to use SIRF's software to build this acquisition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create acquisition model matrix (this uses something called ray tracing, which you can ignore for this example)\n",
    "# This is a 3-D (or in our case 2-D) Radon Transform matrix\n",
    "acq_model_matrix  = pet.RayTracingMatrix()\n",
    "# we will increase the number of rays used for every Line-of-Response (LOR) as an example\n",
    "# (it is not required for the exercise of course)\n",
    "acq_model_matrix.set_num_tangential_LORs(5)\n",
    "# We can now create the acquisition model using this matrix\n",
    "acq_model = pet.AcquisitionModelUsingMatrix(acq_model_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have $\\mathcal{A} \\approx \\mathcal{R} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create the acquisition sensitivity model - a sinogram containing the sensitivity of each LOR. T\n",
    "# This will depend on individual detector efficiencies, the geometry of the scanner, and the attenuation image\n",
    "acq_model_for_attn = pet.AcquisitionModelUsingRayTracingMatrix() # this saves us a line of code but is the same as in the previous cell\n",
    "# We now create the sensitivity model using the acquisition model and the attenuation image\n",
    "asm_attn = pet.AcquisitionSensitivityModel(attn_image, acq_model_for_attn)\n",
    "asm_attn.set_up(template)\n",
    "# we can now find the attenuation sensitivity factors for each LOR by forward projecting a uniform image.\n",
    "# We can set the value of this uniform image to be our detector efficiency. For now, let's just use 1.\n",
    "attn_factors = asm_attn.forward(template.get_uniform_copy(1))\n",
    "plt.figure()\n",
    "plot_2d_image([1,1,1], attn_factors.as_array()[0,0,:], \"LOR attenuation sensitivity factors\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"image\" looks a bit funny. Hopefully you've done some reading into this already, but this is what's know as a sinogram (because of the sinusoidal shape) and consists of 2D views of the object from different angles stacked on top of eachother. It's basicall what our detectors 'see' from a uniform image of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then add the detector sensitivity (based on the attenuation image) that we made previously\n",
    "acq_model.set_acquisition_sensitivity(asm_attn)\n",
    "# set-up\n",
    "acq_model.set_up(template,image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we now have $\\mathcal{A} \\approx \\mathcal{N} A \\mathcal{R} $. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see how this system will forward project our image and use this to simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% simulate some data using forward projection\n",
    "acquired_data=acq_model.forward(image)\n",
    "plot_2d_image([1,1,1], acquired_data.as_array()[0,0,:], \"Acquired data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(proj_data,noise_factor = 0.1, seed = 50):\n",
    "    \"\"\"Add Poission noise to acquisition data.\"\"\"\n",
    "    proj_data_arr = proj_data.as_array() / noise_factor\n",
    "    # Data should be >=0 anyway, but add abs just to be safe\n",
    "    np.random.seed(seed)\n",
    "    noisy_proj_data_arr = np.random.poisson(proj_data_arr).astype('float32');\n",
    "    noisy_proj_data = proj_data.clone()\n",
    "    noisy_proj_data.fill(noisy_proj_data_arr*noise_factor);\n",
    "    return noisy_proj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_data = add_noise(acquired_data,1)\n",
    "plot_2d_image([1,1,1], noisy_data.as_array()[0,0,:], \"Acquired data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK and we have some simulated data. We have added poisson noise because of photon counting statistics where we either detect a count or we don't. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to our sensitivity image. We can either treat our sensitivity such that we correct in the projection data space as above or we can correct in our image space:\n",
    "$$ \\mathcal{A} = \\mathcal{R} A \\mathcal{N} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sensitivity image will look like the backprojection of a uniform sinogram of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_image = acq_model.backward(template.get_uniform_copy(1))\n",
    "plot_2d_image([1,1,1], sens_image.as_array()[0,:], \"sensitivity image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets have a look at what can happen to our sensitivity image (or attenuation factors) if we have a misaligned object. \\\\\n",
    "\n",
    "We'll first create a function to apply an affine transform to our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_transform_2D(theta, tx, ty, sx, sy, image_arr):\n",
    "    ''' create a random affine transformation for 2D images '''\n",
    "    # create the transformation matrix\n",
    "    transformation_matrix = np.array([[sx*np.cos(theta), -sy*np.sin(theta), tx],\n",
    "                                        [sx*np.sin(theta),  sy*np.cos(theta), ty],\n",
    "                                        [0, 0, 1]])\n",
    "\n",
    "    # apply the transformation\n",
    "    image_arr_transformed = affine_transform(image_arr, transformation_matrix, order=1)\n",
    "    return image_arr_transformed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this to out attenuation image and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_attn = attn_image.as_array()\n",
    "\n",
    "# apply a random affine transformation using np.random\n",
    "theta = np.random.uniform(-np.pi/32, np.pi/32)\n",
    "tx, ty = np.random.uniform(-1, 1), np.random.uniform(-1, 1)\n",
    "sx, sy = np.random.uniform(0.98, 1.08), np.random.uniform(0.98, 1.08)\n",
    "vol_transformed = affine_transform_2D(theta, tx, ty, sx, sy, vol_attn[0,:])\n",
    "\n",
    "vol_transformed = np.expand_dims(vol_transformed, axis=0)\n",
    "\n",
    "attn_image_transform = attn_image.clone().fill(vol_transformed)\n",
    "\n",
    "vol_im = image.as_array()\n",
    "vol_transformed = affine_transform_2D(theta, tx, ty, sx, sy, vol_im[0,:])\n",
    "vol_transformed = np.expand_dims(vol_transformed, axis=0)\n",
    "\n",
    "image_transform = image.clone().fill(vol_transformed)\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([1,2,1], attn_image.as_array()[0,:], \"original attenuation image\")\n",
    "plot_2d_image([1,2,2], attn_image_transform.as_array()[0,:], \"new attenuation image\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we have simulated some (large) movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_model_for_attn_transform = pet.AcquisitionModelUsingRayTracingMatrix() # this saves us a line of code but is the same as in the previous cell\n",
    "# We now create the sensitivity model using the acquisition model and the attenuation image\n",
    "asm_attn_transform = pet.AcquisitionSensitivityModel(attn_image_transform, acq_model_for_attn_transform)\n",
    "asm_attn_transform.set_up(template)\n",
    "attn_factors_transform = asm_attn_transform.forward(template.get_uniform_copy(1))\n",
    "# And then add the detector sensitivity (based on the attenuation image) that we made previously\n",
    "acq_model_transform = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_model_transform.set_acquisition_sensitivity(asm_attn_transform)\n",
    "# set-up\n",
    "acq_model_transform.set_up(template,image)\n",
    "sens_image_transform = acq_model_transform.backward(template.get_uniform_copy(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_2d_image([1,2,1], sens_image.as_array()[0,:], \"sensitivity image\")\n",
    "plot_2d_image([1,2,2], sens_image_transform.as_array()[0,:], \"new sensitivity image\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare a few reconstructions with our old and new sensitivity images. We'll use a home-made OSEM to highlight this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's write a quick function to deal with zero division errors outsiude of the FoV. We're using a parallel programming functionality called numba. This can be ignored. We're just setting pixels outside the FoV to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def sensitivity_division(arr1, arr2):\n",
    "    tmp  = np.zeros_like(arr1).flatten()\n",
    "    for i in prange(tmp.size):\n",
    "        if arr2.flatten()[i] != 0:\n",
    "            tmp[i] = arr1.flatten()[i]/arr2.flatten()[i]\n",
    "        else:\n",
    "            tmp[i] = 0\n",
    "    return tmp.reshape(arr1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a function to perform a step of the Maximum Likelihood Expectation Maximisation. This is a reconstruction algorithm (don;t worry too much about where it comes from) with an update equation:\n",
    "$$ u_{n+1} = \\frac{u_n}{A^T \\mathbf{1}} A^T \\frac{y}{A u_n + s + r} $$\n",
    "where, as in Rahmim et al (2004) we have cancelled the attenuation image in the forward and backward steps. \\\\\n",
    "This algorithm does the following:\n",
    "1. We find the ratio of our measured data and our expected data $\\frac{y}{E[y]}$\n",
    "2. we back project this ratio into our image domain $A^T$\n",
    "3. we multiply by the previous image iterate divided by our sensitivity image $\\frac{u_n}{A^T \\mathbf{1}} $\n",
    "This algorithm is used regularly in clinic (most often in its subset form OSEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a single MLEM update\n",
    "def MLEM_step(input_image, acq_model, acquired_data, sensitivity_image):\n",
    "    # forward projection\n",
    "    forward_projected_data = acq_model.forward(input_image)\n",
    "    # divide acquired data by forward projected data\n",
    "    ratio = acquired_data.divide(forward_projected_data)\n",
    "    # back projection\n",
    "    back_projected_data = acq_model.backward(ratio)\n",
    "    # divide by sensitivity image\n",
    "    back_projected_data = back_projected_data.divide(sensitivity_image)\n",
    "    # update input image\n",
    "    output_image = input_image*back_projected_data\n",
    "    return output_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create initial image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a uniform image, cropped to only fill the field of view of the scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image=image.get_uniform_copy(cmax / 4)\n",
    "make_cylindrical_FOV(initial_image)\n",
    "# display\n",
    "im_slice = initial_image.dimensions()[0] // 2\n",
    "plt.figure()\n",
    "plot_2d_image([1,1,1],initial_image.as_array()[im_slice,:,:], 'initial image',[0,cmax])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track how closely our estimate fits the data, we create something called an objective function. We are working with Poisson noise and so we need a data fidelity function that takes this into account. Here we use the poisson log likelihood (this comes from Poisson probability and Bayes theorem - look it up)\n",
    "$$ \\mathcal{D}_{PLL} = y \\ln(E[y]) - E[y] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun = pet.make_Poisson_loglikelihood(noisy_data)\n",
    "obj_fun.set_acquisition_model(acq_model)\n",
    "obj_fun.set_acquisition_data(noisy_data)\n",
    "obj_fun.set_up(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we no longer need attenuation in our acquisition model, we'll use a bog standard discrete random transform operator to do our forward and back projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon_transform = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "radon_transform.set_up(template, image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run same reconstruction but saving images and objective function values every sub-iteration\n",
    "num_iters = 32\n",
    "\n",
    "# create an image object that will be updated during the iterations\n",
    "current_image = initial_image.clone()\n",
    "\n",
    "# create an array to store the values of the objective function at every\n",
    "# sub-iteration (and fill in the first)\n",
    "osem_objective_function_values = [obj_fun.value(current_image)]\n",
    "\n",
    "# create an ndarray to store the images at every sub-iteration\n",
    "all_osem_images = np.ndarray(shape=(num_iters + 1,) + current_image.dimensions())\n",
    "all_osem_images[0,:,:,:] = current_image.as_array()\n",
    "\n",
    "# do the actual updates\n",
    "for i in range(1, num_iters+1):\n",
    "    current_image = MLEM_step(current_image, radon_transform, acquired_data, sens_image)\n",
    "    # store results\n",
    "    obj_fun_value = obj_fun.value(current_image)\n",
    "    osem_objective_function_values.append(obj_fun_value)\n",
    "    all_osem_images[i,:,:,:] =  current_image.as_array()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,1,1], all_osem_images[-1][0,:], \"reconstructed image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some plots with these results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's going on during the reconstruction by plotting a few of the iterates along with the update and the difference between the image estimate and the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define a function for plotting images and the updates\n",
    "def plot_progress(all_images, title, ground_truth, subiterations = [], max = cmax):\n",
    "    ground_truth_array = ground_truth.as_array()\n",
    "    if len(subiterations) == 0:\n",
    "        num_subiters = all_images[0].shape[0] - 1\n",
    "        subiterations = range(1, num_subiters + 1)\n",
    "    num_rows = len(all_images)\n",
    "\n",
    "    for i in subiterations:\n",
    "        plt.figure()\n",
    "        for r in range(num_rows):\n",
    "            plot_2d_image([num_rows,3,3 * r + 1],\n",
    "                          all_images[r][i,im_slice,:,:],'%s at %d' % (title[r], i), [0,max])\n",
    "            plot_2d_image([num_rows,3,3*r+2],\n",
    "                          all_images[r][i,im_slice,:,:]-all_images[r][i - 1,im_slice,:,:],'update',[-max*.05,max*.05], cmap='seismic')\n",
    "            plot_2d_image([num_rows,3,3*r+3],\n",
    "                          all_images[r][i,im_slice,:,:]-ground_truth_array[im_slice,:,:],'error',[-max*.5,max*.5], cmap='seismic')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% now call this function to see how we went along\n",
    "# note that in the notebook interface, this might create a box with a vertical slider\n",
    "subiterations = (1,2,4,8,16,24,32)\n",
    "# close all \"open\" images as otherwise we will get warnings (the notebook interface keeps them \"open\" somehow)\n",
    "plt.close('all')    \n",
    "plot_progress([all_osem_images], ['MLEM'], image, subiterations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see what progess we make by plotting the value of our objective function for each iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot objective function values\n",
    "plt.figure()\n",
    "#plt.plot(subiterations, [ osem_objective_function_values[i] for i in subiterations])\n",
    "plt.plot(osem_objective_function_values)\n",
    "plt.title('Objective function values')\n",
    "plt.xlabel('sub-iterations');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. so let's now compare this to a reconstruction with the offset image, but with the original sensitivity image. We'll then compare this to a reconstruction with the correct sensitivity image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_data_transform = add_noise(acq_model_transform.forward(image_transform),1) # new noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun_transform = pet.make_Poisson_loglikelihood(noisy_data_transform)\n",
    "obj_fun_transform.set_acquisition_model(acq_model_transform)\n",
    "obj_fun_transform.set_acquisition_data(noisy_data_transform)\n",
    "obj_fun_transform.set_up(image_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the incorrect sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run same reconstruction but saving images and objective function values every sub-iteration\n",
    "num_iters = 32\n",
    "\n",
    "# create an image object that will be updated during the iterations\n",
    "current_image = initial_image.clone()\n",
    "\n",
    "# create an array to store the values of the objective function at every\n",
    "# sub-iteration (and fill in the first)\n",
    "osem_objective_function_values_transform = [obj_fun.value(current_image)]\n",
    "\n",
    "# create an ndarray to store the images at every sub-iteration\n",
    "all_osem_images_transform = np.ndarray(shape=(num_iters + 1,) + current_image.dimensions())\n",
    "all_osem_images_transform[0,:,:,:] = current_image.as_array()\n",
    "\n",
    "# do the actual updates\n",
    "for i in range(1, num_iters+1):\n",
    "    current_image = MLEM_step(current_image, radon_transform, noisy_data_transform, sens_image)\n",
    "    # store results\n",
    "    obj_fun_value = obj_fun_transform.value(current_image)\n",
    "    osem_objective_function_values_transform.append(obj_fun_value)\n",
    "    all_osem_images_transform[i,:,:,:] =  current_image.as_array()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the correct sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run same reconstruction but saving images and objective function values every sub-iteration\n",
    "num_iters = 32\n",
    "\n",
    "# create an image object that will be updated during the iterations\n",
    "current_image = initial_image.clone()\n",
    "\n",
    "# create an array to store the values of the objective function at every\n",
    "# sub-iteration (and fill in the first)\n",
    "osem_objective_function_values_transform_corr = [obj_fun.value(current_image)]\n",
    "\n",
    "# create an ndarray to store the images at every sub-iteration\n",
    "all_osem_images_transform_corr = np.ndarray(shape=(num_iters + 1,) + current_image.dimensions())\n",
    "all_osem_images_transform_corr[0,:,:,:] = current_image.as_array()\n",
    "\n",
    "# do the actual updates\n",
    "for i in range(1, num_iters+1):\n",
    "    current_image = MLEM_step(current_image, radon_transform, noisy_data_transform, sens_image_transform)\n",
    "    # store results\n",
    "    obj_fun_value = obj_fun_transform.value(current_image)\n",
    "    osem_objective_function_values_transform_corr.append(obj_fun_value)\n",
    "    all_osem_images_transform_corr[i,:,:,:] =  current_image.as_array()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what difference a correct sensitivity made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,3,1], all_osem_images[-1][0,:], \"original image\", clims = [0,200])\n",
    "plot_2d_image([1,3,2], all_osem_images_transform[-1][0,:], \"incorr sens im\", clims = [0,200])\n",
    "plot_2d_image([1,3,3], all_osem_images_transform_corr[-1][0,:], \"corr sens im\", clims = [0,200])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shown us that there can be a large deviation in reconstructed image if the sensitivity image is incorrect. So how can we fix this? One method is to `gate` our acquisition into groups of timepoint with similar attenuation maps (i.e patient positions). We can then calculate a sensitivity image for all these timepoints. However, this could lead to long calculation times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could therefore use a neural network to output a the required change in a sensitivity image based on the change in an attenuation image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a file that can generate the shepp logan phantom\n",
    "from odl_funcs.ellipses import EllipsesDataset\n",
    "# Import standard extra packages\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.optim as optim\n",
    "\n",
    "mini_batch = 10\n",
    "from sirf.Utilities import examples_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odl_funcs.misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\" \n",
    "    A CNN for PET sensitivity estimation.\n",
    "    Consists of 3 convolutional layers with ReLU activation. \n",
    "    Kernel sizes are 15x15, 9x9, 5x5, 3x3, 3x3, 7x7, 15x15, 3x3, 3x3 with padding \n",
    "    The idea of this is to have a large kernel size at the beginning to capture\n",
    "    the large motion, and then gradually reduce the kernel size to capture the\n",
    "    smaller motion.\n",
    "    The kernel sizes then increases again to provide an edge filtering effect on a blurry image\n",
    "    A ReLU activation is used after each convolutional layer.\n",
    "    Input:\n",
    "        2 images (sensitivity with no motion and attenuation correction map)\n",
    "    Output:\n",
    "        1 image (sensitivity with motion)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 16, 15, padding=7)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 9, padding=4)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(64, 32, 7, padding=3)\n",
    "        self.conv6 = nn.Conv2d(32, 16, 15, padding=7)\n",
    "        self.conv7 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(16, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x[:,1,:,:]=x[:,1,:,:].mul(10)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = 2\n",
    "train_dataloader = torch.utils.data.DataLoader( \\\n",
    "    EllipsesDataset(radon_transform, attn_image, template, mode=\"train\", n_samples = 16) \\\n",
    "    , batch_size=mini_batch, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader( \\\n",
    "    EllipsesDataset(radon_transform, attn_image, template, mode=\"valid\", n_samples = 4) \\\n",
    "    , batch_size=mini_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = SimpleCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0000001) # really small learning rate because we should be (almost) at the optimum already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/home/sam/working/SIRF-Exercises/notebooks/PET_sensitivity/model_cnn_large.pt\", map_location=device))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "for epoch in range(3): # 3full passes over the data\n",
    "    for data, validation in zip(train_dataloader, valid_dataloader):  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        model.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = model(X.to(device))  # pass in the reshaped batch\n",
    "        loss = loss_function(output, y.to(device))  # calc and grab the loss value\n",
    "        train_loss_history.append(loss)\n",
    "        #valid_loss_history.append(loss_function(net(validation[0].to(device)), validation[1].to(device)))\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history=[]\n",
    "for i in train_loss_history:\n",
    "    loss_history.append(i.item())\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was already converged, which is why you see this flat, noisy line here. The training objective function looked more like this (note we're minimising the objective function for the network - this is just a converntion thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('/home/sam/working/SIRF-Exercises/notebooks/PET_sensitivity/loss_history_CNN_large.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.from_numpy(np.squeeze(np.array([sens_image.as_array(), attn_image_transform.as_array()]))).unsqueeze(0)\n",
    "input_data = input_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_image_NN = sens_image.clone().fill(out.detach().cpu().numpy()[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,3,1], sens_image.as_array()[0,:,:], 'sens im', cmap=\"viridis\")\n",
    "plot_2d_image([1,3,2], sens_image_transform.as_array()[0,:,:], 'sens im trans', cmap=\"viridis\")\n",
    "plot_2d_image([1,3,3], sens_image_NN.as_array()[0,:,:], 'sens im corr', cmap=\"viridis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that was surprisingly good! There's definitely a lot wrong, but you can se the network was trying to do what we asked - how could we improve this network to more accurately predict out sensitivity image?\n",
    "\n",
    "Let's see how this looks when we reconstruct it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run same reconstruction but saving images and objective function values every sub-iteration\n",
    "num_iters = 32\n",
    "\n",
    "# create an image object that will be updated during the iterations\n",
    "current_image_NN = initial_image.clone()\n",
    "\n",
    "# create an array to store the values of the objective function at every\n",
    "# sub-iteration (and fill in the first)\n",
    "osem_objective_function_values_NN = [obj_fun.value(current_image_NN)]\n",
    "\n",
    "# create an ndarray to store the images at every sub-iteration\n",
    "all_osem_images_NN = np.ndarray(shape=(num_iters + 1,) + current_image_NN.dimensions())\n",
    "all_osem_images_NN[0,:,:,:] = current_image_NN.as_array()\n",
    "\n",
    "# do the actual updates\n",
    "for i in range(1, num_iters+1):\n",
    "    current_image_NN = MLEM_step(current_image_NN, radon_transform, noisy_data_transform, sens_image_NN)\n",
    "    # store results\n",
    "    all_osem_images_NN[i,:,:,:] = current_image_NN.as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([2,3,1], all_osem_images_NN[-1,0,:,:], 'Neural Net', cmap=\"viridis\", clims=[0,150])\n",
    "plot_2d_image([2,3,2], all_osem_images_transform[-1,0,:,:], 'incorrect map', cmap=\"viridis\", clims=[0,150])\n",
    "plot_2d_image([2,3,3], all_osem_images_transform_corr[-1,0,:,:], 'Gold Standard', cmap=\"viridis\", clims=[0,150])\n",
    "plot_2d_image([2,3,4], all_osem_images_NN[-1,0,:,:]-all_osem_images_transform_corr[-1,0,:,:], 'diff', cmap=\"viridis\")\n",
    "plot_2d_image([2,3,5], all_osem_images_transform[-1,0,:,:]-all_osem_images_transform_corr[-1,0,:,:], 'diff', cmap=\"viridis\")\n",
    "plot_2d_image([2,3,6], all_osem_images_transform_corr[-1,0,:,:]-all_osem_images_transform_corr[-1,0,:,:], 'diff', cmap=\"viridis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's not great, but this is just a start!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to think about\n",
    "## What network architecture is best? A UNet, maybe?\n",
    "## We could try using difference images. How would we need to change the network (would a ReLu work?)\n",
    "## Is the the best loss function (SSIM, MAE, L1 loss, smoothed loss)\n",
    "## Will more training data fix our issue?\n",
    "## Are ellipses really the best option for training (Brainweb training data?)\n",
    "## Can we simulate non-affine transforms (yawning etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And many more..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

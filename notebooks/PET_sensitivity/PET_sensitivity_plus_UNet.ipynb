{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on <em> Demonstration of maximum-likelihood reconstruction with SIRF <\\em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% make sure figures appears inline and animations works\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup the working directory for the notebook\n",
    "import notebook_setup\n",
    "from sirf_exercises import cd_to_working_dir\n",
    "cd_to_working_dir('PET', 'ML_reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Initial imports etc\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from numba import njit, prange\n",
    "#import scipy\n",
    "#from scipy import optimize\n",
    "import sirf.STIR as pet\n",
    "import sirf.Reg as reg\n",
    "from sirf.Utilities import examples_data_path\n",
    "from sirf_exercises import exercises_data_path\n",
    "pet.set_verbosity(0)\n",
    "\n",
    "# define the directory with input files for this notebook\n",
    "data_path = os.path.join(examples_data_path('PET'), 'thorax_single_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up redirection of STIR messages to files\n",
    "msg_red = pet.MessageRedirector('info.txt', 'warnings.txt', 'errors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% some handy function definitions\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink=.4)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def make_positive(image_array):\n",
    "    \"\"\"truncate any negatives to zero\"\"\"\n",
    "    image_array[image_array<0] = 0\n",
    "    return image_array\n",
    "\n",
    "def make_cylindrical_FOV(image):\n",
    "    \"\"\"truncate to cylindrical FOV\"\"\"\n",
    "    filter = pet.TruncateToCylinderProcessor()\n",
    "    filter.apply(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some simulated data from ground-truth images\n",
    "This is a repetition of the code in the OSEM notebook, just such that the current notebook is self-contained. However, there are no explanations here.\n",
    "\n",
    "You should be able to adapt the notebook to use your own data as well of course. The actual reconstruction exercises and its evaluation does not require that the input is a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Read in images\n",
    "image = pet.ImageData(os.path.join(data_path, 'emission.hv'))*0.05\n",
    "attn_image = pet.ImageData(os.path.join(data_path, 'attenuation.hv'))\n",
    "template = pet.AcquisitionData(os.path.join(data_path, 'template_sinogram.hs'))\n",
    "plt.figure()\n",
    "plot_2d_image([1,2,1], image.as_array()[0,:], \"image\")\n",
    "plot_2d_image([1,2,2], attn_image.as_array()[0,:], \"attenuation image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% save max for future displays\n",
    "cmax = image.max()*.6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start creating our detector model. A PET (or SPECT or CT or...) acquisition process is characterised by a system matrix, $\\mathcal{A}$, as well as additive contributions consisting of scatter and random coincidences:\n",
    "$$f = \\mathcal{A}u + s + r $$\n",
    "Wwhere $f$ is our data, $u$ is our image and $s,r$ are our additive componatnts. We will only model the system matrix in this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our system model is itself comprised of a number of different operations. We will concentrate on three of these: the radon transform, $\\mathcal{R}$, attenuation, $A$, and detector normalisation $\\mathcal{N}$, giving us:\n",
    "$$ \\mathcal{A} \\approx \\mathcal{N} A \\mathcal{R} $$\n",
    "where attenuation corrections and normalisation and multiplicative factors in the projection domain. \\\n",
    "And so our next job is to use SIRF's software to build this acquisition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create acquisition model matrix (this uses something called ray tracing, which you can ignore for this example)\n",
    "# This is a 3-D (or in our case 2-D) Radon Transform matrix\n",
    "acq_model_matrix  = pet.RayTracingMatrix()\n",
    "# we will increase the number of rays used for every Line-of-Response (LOR) as an example\n",
    "# (it is not required for the exercise of course)\n",
    "acq_model_matrix.set_num_tangential_LORs(5)\n",
    "# We can now create the acquisition model using this matrix\n",
    "acq_model = pet.AcquisitionModelUsingMatrix(acq_model_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have $\\mathcal{A} \\approx \\mathcal{R} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create the acquisition sensitivity model - a sinogram containing the sensitivity of each LOR. T\n",
    "# This will depend on individual detector efficiencies, the geometry of the scanner, and the attenuation image\n",
    "acq_model_for_attn = pet.AcquisitionModelUsingRayTracingMatrix() # this saves us a line of code but is the same as in the previous cell\n",
    "# We now create the sensitivity model using the acquisition model and the attenuation image\n",
    "asm_attn = pet.AcquisitionSensitivityModel(attn_image, acq_model_for_attn)\n",
    "asm_attn.set_up(template)\n",
    "# we can now find the attenuation sensitivity factors for each LOR by forward projecting a uniform image.\n",
    "# We can set the value of this uniform image to be our detector efficiency. For now, let's just use 1.\n",
    "attn_factors = asm_attn.forward(template.get_uniform_copy(1))\n",
    "plt.figure()\n",
    "plot_2d_image([1,1,1], attn_factors.as_array()[0,0,:], \"LOR attenuation sensitivity factors\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"image\" looks a bit funny. Hopefully you've done some reading into this already, but this is what's know as a sinogram (because of the sinusoidal shape) and consists of 2D views of the object from different angles stacked on top of eachother."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular sinogram is showing the sensitivity for each line of response between two detectors due to the attenuation of the object for imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And so let's use this in our sensitivity model\n",
    "asm_attn = pet.AcquisitionSensitivityModel(attn_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then add the detector sensitivity (based on the attenuation image) that we made previously\n",
    "acq_model.set_acquisition_sensitivity(asm_attn)\n",
    "# set-up\n",
    "acq_model.set_up(template,image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we now have $\\mathcal{A} \\approx \\mathcal{N} A \\mathcal{R} $. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then see how this system will forward project our image and use this to simulate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% simulate some data using forward projection\n",
    "acquired_data=acq_model.forward(image)\n",
    "plot_2d_image([1,1,1], acquired_data.as_array()[0,0,:], \"Acquired data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(proj_data,noise_factor = 0.1, seed = 50):\n",
    "    \"\"\"Add Poission noise to acquisition data.\"\"\"\n",
    "    proj_data_arr = proj_data.as_array() / noise_factor\n",
    "    # Data should be >=0 anyway, but add abs just to be safe\n",
    "    np.random.seed(seed)\n",
    "    noisy_proj_data_arr = np.random.poisson(proj_data_arr).astype('float32');\n",
    "    noisy_proj_data = proj_data.clone()\n",
    "    noisy_proj_data.fill(noisy_proj_data_arr*noise_factor);\n",
    "    return noisy_proj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquired_data = add_noise(acquired_data)\n",
    "plot_2d_image([1,1,1], acquired_data.as_array()[0,0,:], \"Acquired data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK and we have some simulated data. We have added poisson noise because of photon counting statistics where we either detect a count or we don't. An example of a poisson distribution with a mean of 2 and 10,000 counts is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 1000\n",
    "data = np.random.poisson(5, num_data)\n",
    "poisson_fit = scipy.stats.poisson.pmf(5, np.arange(0, 20))*num_data\n",
    "plt.hist(data)\n",
    "plt.plot(poisson_fit)\n",
    "np.max(poisson_fit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to our sensitivity image. We can either treat our sensitivity such that we correct in the projection data space as above or we can correct in our image space:\n",
    "$$ \\mathcal{A} = \\mathcal{R} A \\mathcal{N} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sensitivity image will look like the backprojection of a uniform sinogram of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens_image = acq_model.backward(template.get_uniform_copy(1))\n",
    "plot_2d_image([1,1,1], sens_image.as_array()[0,:], \"sensitivity image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets have a look at what can happen to our sensitivity image (or attenuation factors) if we have a misaligned object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_geom_info = attn_image.get_geometrical_info()\n",
    "A_LPH = s_geom_info.get_index_to_physical_point_matrix()  # 4x4 affine matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = attn_image.as_array()\n",
    "\n",
    "vol_new = np.roll(vol, 5, axis = 1)\n",
    "vol_new = np.roll(vol_new, 5, axis = 2)\n",
    "attn_image_new = attn_image.clone().fill(vol_new)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plot_2d_image([1,2,1], attn_image.as_array()[0,:], \"original attenuation image\")\n",
    "plot_2d_image([1,2,2], attn_image_new.as_array()[0,:], \"new attenuation image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_model_for_attn_new = pet.AcquisitionModelUsingRayTracingMatrix() # this saves us a line of code but is the same as in the previous cell\n",
    "# We now create the sensitivity model using the acquisition model and the attenuation image\n",
    "asm_attn_new = pet.AcquisitionSensitivityModel(attn_image_new, acq_model_for_attn_new)\n",
    "asm_attn_new.set_up(template)\n",
    "attn_factors_new = asm_attn_new.forward(template.get_uniform_copy(1))\n",
    "# And then add the detector sensitivity (based on the attenuation image) that we made previously\n",
    "acq_model_new = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "acq_model_new.set_acquisition_sensitivity(asm_attn_new)\n",
    "# set-up\n",
    "acq_model_new.set_up(template,image)\n",
    "sens_image_new = acq_model_new.backward(template.get_uniform_copy(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_2d_image([1,2,1], sens_image.as_array()[0,:], \"sensitivity image\")\n",
    "plot_2d_image([1,2,2], sens_image_new.as_array()[0,:], \"new sensitivity image\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare a few reconstructions with our old and new sensitivity images. We'll use a home-made OSEM to highlight this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's write a quick function to deal with zero division errors outsiude of the FoV. We're using a parallel programming functionality called numba. This can be ignored. We're just setting pixels outside the FoV to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_division(arr1, arr2):\n",
    "    tmp  = np.zeros_like(arr1).flatten()\n",
    "    for i in prange(tmp.size):\n",
    "        if arr2.flatten()[i] != 0:\n",
    "            tmp[i] = arr1.flatten()[i]/arr2.flatten()[i]\n",
    "        else:\n",
    "            tmp[i] = 0\n",
    "    return tmp.reshape(arr1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a function to perform a step of the Maximum Likelihood Expectation Maximisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs a single MLEM update\n",
    "def MLEM_step(input_image, acq_model, acquired_data, sensitivity_image_array):\n",
    "    # forward projection\n",
    "    forward_projected_data = acq_model.forward(input_image)\n",
    "    # divide acquired data by forward projected data\n",
    "    ratio = acquired_data / forward_projected_data\n",
    "    # back projection\n",
    "    back_projected_data = acq_model.backward(ratio).as_array()\n",
    "    # divide by sensitivity image\n",
    "    back_projected_data_array = sensitivity_division(back_projected_data, sensitivity_image_array)\n",
    "    # update input image\n",
    "    output_image = input_image*input_image.clone().fill(back_projected_data_array)\n",
    "    return output_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create initial image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous OSEM notebook, we just used a uniform image. Here, we will use a disk that roughly corresponds to the *Field of View (FOV)*. The reason for this is that it makes things easier for display and the gradient ascent code below.\n",
    "\n",
    "An alternative solution would be to tell the `acq_model` to use a square FOV as opposed to a circular one, but that will slow down calculations just a little bit, so we won't do that here (feel free to try!).\n",
    "\n",
    "In addition, the initial value is going to be a bit more important here as we're going to plot the value of the objective function. Obviously, having a descent estimate of the scale of the image will make that plot look more sensible. Feel free to experiment with the value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_image=image.get_uniform_copy(cmax / 4)\n",
    "make_cylindrical_FOV(initial_image)\n",
    "# display\n",
    "im_slice = initial_image.dimensions()[0] // 2\n",
    "plt.figure()\n",
    "plot_2d_image([1,1,1],initial_image.as_array()[im_slice,:,:], 'initial image',[0,cmax])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fun = pet.make_Poisson_loglikelihood(acquired_data)\n",
    "obj_fun.set_acquisition_model(acq_model)\n",
    "obj_fun.set_acquisition_data(acquired_data)\n",
    "obj_fun.set_up(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon_transform = pet.AcquisitionModelUsingRayTracingMatrix()\n",
    "radon_transform.set_up(template, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run same reconstruction but saving images and objective function values every sub-iteration\n",
    "num_iters = 32\n",
    "\n",
    "# create an image object that will be updated during the iterations\n",
    "current_image = initial_image.clone()\n",
    "\n",
    "# create an array to store the values of the objective function at every\n",
    "# sub-iteration (and fill in the first)\n",
    "osem_objective_function_values = [obj_fun.value(current_image)]\n",
    "\n",
    "# create an ndarray to store the images at every sub-iteration\n",
    "all_osem_images = np.ndarray(shape=(num_iters + 1,) + current_image.dimensions())\n",
    "all_osem_images[0,:,:,:] = current_image.as_array()\n",
    "\n",
    "# do the actual updates\n",
    "for i in range(1, num_iters+1):\n",
    "    current_image = MLEM_step(current_image, radon_transform, acquired_data, sens_image.as_array())\n",
    "    # store results\n",
    "    obj_fun_value = obj_fun.value(current_image)\n",
    "    osem_objective_function_values.append(obj_fun_value)\n",
    "    all_osem_images[i,:,:,:] =  current_image.as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,1,1], all_osem_images[-1][0,:], \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some plots with these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define a function for plotting images and the updates\n",
    "def plot_progress(all_images, title, subiterations = []):\n",
    "    if len(subiterations) == 0:\n",
    "        num_subiters = all_images[0].shape[0] - 1\n",
    "        subiterations = range(1, num_subiters + 1)\n",
    "    num_rows = len(all_images)\n",
    "\n",
    "    for i in subiterations:\n",
    "        plt.figure()\n",
    "        for r in range(num_rows):\n",
    "            plot_2d_image([num_rows,2,2 * r + 1],\n",
    "                          all_images[r][i,im_slice,:,:],'%s at %d' % (title[r], i), [0,cmax])\n",
    "            plot_2d_image([num_rows,2,2*r+2],\n",
    "                          all_images[r][i,im_slice,:,:]-all_images[r][i - 1,im_slice,:,:],'update',[-cmax*.05,cmax*.05], cmap='seismic')\n",
    "        #plt.pause(.05)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% now call this function to see how we went along\n",
    "# note that in the notebook interface, this might create a box with a vertical slider\n",
    "subiterations = (1,2,4,8,16,24,32)\n",
    "# close all \"open\" images as otherwise we will get warnings (the notebook interface keeps them \"open\" somehow)\n",
    "plt.close('all')    \n",
    "plot_progress([all_osem_images], ['MLEM'],subiterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot objective function values\n",
    "plt.figure()\n",
    "#plt.plot(subiterations, [ osem_objective_function_values[i] for i in subiterations])\n",
    "plt.plot(osem_objective_function_values)\n",
    "plt.title('Objective function values')\n",
    "plt.xlabel('sub-iterations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot seems to indicate that (OS)EM converges to a stable value of the\n",
    "log-likelihood very quickly. However, as we've seen, the images are still changing.\n",
    "\n",
    "Convince yourself that the likelihood is still increasing (either by zooming into the figure, or by using `plt.ylim`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute some simple ROI values as well. Let's plot those.\n",
    "\n",
    "You might want to convince yourself first that these ROI are in the correct place (but it doesn't matter too much for this exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% ROI\n",
    "ROI_lesion = all_osem_images[:,(im_slice,), 65:70, 40:45]\n",
    "ROI_lung = all_osem_images[:,(im_slice,), 75:80, 45:50]\n",
    "\n",
    "ROI_mean_lesion = ROI_lesion.mean(axis=(1,2,3))\n",
    "ROI_std_lesion = ROI_lesion.std(axis=(1,2,3))\n",
    "\n",
    "ROI_mean_lung = ROI_lung.mean(axis=(1,2,3))\n",
    "ROI_std_lung = ROI_lung.std(axis=(1,2,3))\n",
    "\n",
    "plt.figure()\n",
    "#plt.hold('on')\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ROI_mean_lesion,'k',label='lesion')\n",
    "plt.plot(ROI_mean_lung,'r',label='lung')\n",
    "plt.legend()\n",
    "plt.title('ROI mean')\n",
    "plt.xlabel('sub-iterations')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ROI_std_lesion, 'k',label='lesion')\n",
    "plt.plot(ROI_std_lung, 'r',label='lung')\n",
    "plt.legend()\n",
    "plt.title('ROI standard deviation')\n",
    "plt.xlabel('sub-iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_2d_image([1,2,1], ROI_lesion[1][0,:], \"lesion\")\n",
    "plot_2d_image([1,2,2], ROI_lung[1][0,:], \"lung\")\n",
    "plt.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. so let's now compare this to a reconstruction with the offset sensitivity iamge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% run same reconstruction but saving images and objective function values every sub-iteration\n",
    "num_iters = 32\n",
    "\n",
    "# create an image object that will be updated during the iterations\n",
    "current_image = initial_image.clone()\n",
    "\n",
    "# create an array to store the values of the objective function at every\n",
    "# sub-iteration (and fill in the first)\n",
    "osem_objective_function_values_new = [obj_fun.value(current_image)]\n",
    "\n",
    "# create an ndarray to store the images at every sub-iteration\n",
    "all_osem_images_new = np.ndarray(shape=(num_iters + 1,) + current_image.dimensions())\n",
    "all_osem_images_new[0,:,:,:] = current_image.as_array()\n",
    "\n",
    "# do the actual updates\n",
    "for i in range(1, num_iters+1):\n",
    "    current_image = MLEM_step(current_image, radon_transform, acquired_data, sens_image_new.as_array())\n",
    "    # store results\n",
    "    obj_fun_value = obj_fun.value(current_image)\n",
    "    osem_objective_function_values_new.append(obj_fun_value)\n",
    "    all_osem_images_new[i,:,:,:] =  current_image.as_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plot objective function values\n",
    "plt.figure()\n",
    "#plt.hold('on')\n",
    "plt.title('Objective function value vs subiterations')\n",
    "plt.plot(osem_objective_function_values_new,'b')\n",
    "plt.plot(osem_objective_function_values,'r')\n",
    "plt.legend(('OSEM with misalignment', 'OSEM'),loc='lower right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,1,1], all_osem_images_new[-1][0,:], \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compare GA and OSEM images\n",
    "plot_progress([all_osem_images_new, all_osem_images], ['OSEM misaligned' ,'OSEM'],[2,4,8,16,32])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has shown us that there can be a large deviation in reconstructed image if the sensitivity image is incorrect. So how can we fix this? One method is to `gate` our acquisition into groups of timepoint with similar attenuation maps (i.e patient positions). We can then calculate a sensitivity image for all these timepoints. However, this could lead to long calculation times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could therefore use a neural network to output a the required change in a sensitivity image based on the change in an attenuation image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a file that can generate the shepp logan phantom\n",
    "from odl_funcs.ellipses import EllipsesDataset\n",
    "# Import standard extra packages\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import torchvision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mini_batch = 10\n",
    "from sirf.Utilities import examples_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odl_funcs.misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" \n",
    "    A block of two convolutional layers with ReLU activation. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 5, padding = (2,2), padding_mode = 'reflect')\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 5, padding = (2,2), padding_mode = 'reflect')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv2(self.relu(self.conv1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder of the U-Net.\n",
    "    \"\"\"\n",
    "    def __init__(self, chs=(2,64,128,256)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) # encoder blocks\n",
    "        self.pool       = nn.MaxPool2d(2) # pooling layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Decoder of the U-Net.\n",
    "    \"\"\"\n",
    "    def __init__(self, chs=(256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs # number of channels\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)]) # upconvolutions\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) # decoder blocks\n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        \"\"\" Crops the encoder features to the size of the decoder features. \"\"\"\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\" \n",
    "    U-Net architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_chs=(2,64,128,256), dec_chs=(256, 128, 64), num_class=1, retain_dim=True, out_sz=(155,155)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs) # encoder\n",
    "        self.decoder     = Decoder(dec_chs) # decoder\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1, padding= (0,0)) # head\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz      = out_sz\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x) # encoder features\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:]) # decoder features\n",
    "        out      = self.head(out) # head\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = 2\n",
    "train_dataloader = torch.utils.data.DataLoader( \\\n",
    "    EllipsesDataset(radon_transform, attn_image, template, mode=\"train\", n_samples = 20) \\\n",
    "    , batch_size=mini_batch, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader( \\\n",
    "    EllipsesDataset(radon_transform, attn_image, template, mode=\"valid\", n_samples = 6) \\\n",
    "    , batch_size=mini_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/home/sam/working/SIRF-Exercises/notebooks/PET_sensitivity/UNet_full.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "model.eval()\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "for epoch in range(3): # 3full passes over the data\n",
    "    for data, validation in zip(train_dataloader, valid_dataloader):  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.cpu())  # pass in the reshaped batch\n",
    "        loss = loss_function(output, y.cpu())  # calc and grab the loss value\n",
    "        train_loss_history.append(loss)\n",
    "        #valid_loss_history.append(loss_function(net(validation[0].to(device)), validation[1].to(device)))\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history=[]\n",
    "for i in train_loss_history:\n",
    "    loss_history.append(i.item())\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was already converged, which is why you see this flat, noisy line here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(np.squeeze(np.array([sens_image.as_array(), attn_image_new.as_array()]))).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(torch.Tensor(np.squeeze(np.array([sens_image.as_array(), attn_image_new.as_array()]))).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,2,1],attn_image.as_array()[0,:,:], \"original\")\n",
    "plot_2d_image([1,2,2],attn_image_new.as_array()[0,:,:], \"new attenuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,1,1],sens_image.as_array()[0,:,:], \"fixed sensitivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,1,1],out[0,0,:,:].detach().numpy(), \"network output sensitivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_image([1,1,1],sens_image_new.as_array()[0,:,:], \"correct sensitivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, our network hasn't really worked. This could be an issue with the architecture, the data or, more than likely, both of these and more! Luckily we have plenty of time to mess around with these things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

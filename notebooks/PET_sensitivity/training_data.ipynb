{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's have a look at the training data that we've used to train our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sirf functionality\n",
    "from sirf.STIR import AcquisitionSensitivityModel, AcquisitionModelUsingRayTracingMatrix, ImageData, AcquisitionData, MessageRedirector\n",
    "from sirf.Utilities import examples_data_path # for our example data\n",
    "\n",
    "# DataLoaders for Pytorch and some of the functions used\n",
    "from odl_funcs.ellipses import EllipsesDataset\n",
    "from odl_funcs.misc import random_phantom, affine_transform_2D_image, affine_transform_2D\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for path handling\n",
    "import os\n",
    "\n",
    "# pytorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up redirection of STIR messages to files\n",
    "msg_red = MessageRedirector('info.txt', 'warnings.txt', 'errors.txt')\n",
    "\n",
    "# big plots\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some handy function definitions\n",
    "def plot_2d_image(idx,vol,title,clims=None,cmap=\"viridis\"):\n",
    "    \"\"\"Customized version of subplot to plot 2D image\"\"\"\n",
    "    plt.subplot(*idx)\n",
    "    plt.imshow(vol,cmap=cmap)\n",
    "    if not clims is None:\n",
    "        plt.clim(clims)\n",
    "    plt.colorbar(shrink=.4)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simulate some random body-ish phantoms, I have decided to use random ellipses. These look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_image(image):\n",
    "    \"\"\"Normalise image to range [0,1]\"\"\"\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image_array = normalise_image(random_phantom((1, 155,155), 25))*2*0.096 # this uses a pseuso-2D shape hence the 1 in the first dimension\n",
    "plot_2d_image([1,1,1], random_image_array[0], \"Random image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK so how do we train our network with this? We need to make ourselves a sensitivity image. The following is taken from the EllipsesDataset class in the odl_funs directory (it's been edited a bit). It takes an attenuation image and returns a sensitivity image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon_transform = AcquisitionModelUsingRayTracingMatrix()\n",
    "\n",
    "def get_sensitivity(attenuation_image, template):\n",
    "    # Forward project image then add noise\n",
    "    acq_model = AcquisitionModelUsingRayTracingMatrix()\n",
    "    asm_attn = AcquisitionSensitivityModel(attenuation_image, radon_transform)\n",
    "    asm_attn.set_up(template)\n",
    "    acq_model.set_acquisition_sensitivity(asm_attn)\n",
    "    acq_model.set_up(template, attenuation_image)\n",
    "    y = acq_model.backward(template.get_uniform_copy(1.0))\n",
    "    return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set up out acquisition model, we need a template sinogram in order to tell sirf what kind of scanner we'll be using (e.g number of projections, voxel sizes...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(examples_data_path('PET'), 'thorax_single_slice')\n",
    "template_sino = AcquisitionData(os.path.join(data_path, 'template_sinogram.hs'))\n",
    "template_image = ImageData(os.path.join(data_path, 'emission.hv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK so let's now construct a SIRF ImageData Object with out random ellipse array from our template image. We'll then find the sensitivity image and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = template_image.fill(random_image_array)\n",
    "random_sens_image = get_sensitivity(random_image, template_sino)\n",
    "plot_2d_image([1,1,1], random_sens_image.as_array()[0], \"Random sensitivity image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right. So next we need some training data with a transformation. In order to do this we''l use an affine tranformation from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate, scale, translate\n",
    "theta, tx, ty, sx, sy = 0.1, 0.1, 0.1, 0.95, 0.95\n",
    "transformed_image = affine_transform_2D_image(theta, tx, ty, sx, sy, random_image)\n",
    "plot_2d_image([1,1,1], transformed_image.as_array()[0], \"Transformed image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now find our sensitivity image to find the target data for training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_sens_image = get_sensitivity(transformed_image, template_sino)\n",
    "plot_2d_image([1,1,1], transformed_sens_image.as_array()[0], \"Transformed sensitivity image\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what is happening in our Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader( \\\n",
    "    EllipsesDataset(radon_transform, random_image, template_sino, mode=\"train\", n_samples = 1) \\\n",
    "    , batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:  # `data` is a batch of data\n",
    "    X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "    plot_2d_image([1,3,1], X[0,0,:,:].numpy(), \"Input sens im\")\n",
    "    plot_2d_image([1,3,2], X[0,1,:,:].numpy(), \"Trans CT im\")\n",
    "    plot_2d_image([1,3,3], y[0,0,:,:].numpy(), \"Target sens im\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we input a sensitivity image and a transformed CT image and output a transformed sensitivity image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to think about\n",
    "- Is this the best training data (how useful is a bunch of ellipses?) - I actually have a few more functions that could be useful, but I'll let you think about this first.\n",
    "- Is this the best input / output (can we use difference images? How will our model have to change?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
